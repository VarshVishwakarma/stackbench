STACKBENCH â€” Multi-Agent Framework Comparator
AI-Powered Â· Evidence-Driven Â· Parallel Â· Developer-Focused

StackBench is a next-generation multi-agent research copilot that analyzes developer frameworks using:

âš™ï¸ Real GitHub Metrics

ğŸ“˜ Clean Wikipedia Summaries with Smart Fallbacks

ğŸ§  LLM-Reasoned Analysis via Groq + Gemini

ğŸ¤ Agent-to-Agent Collaboration

ğŸ“¡ Real-Time Logs, Metrics & Parallel Timeline Visualization

It delivers fast, verifiable, and architect-grade recommendations â€” all inside a modern Streamlit UI.

ğŸš€ Why StackBench? â€” The Problem

Picking the right technology is a minefield:

âŒ Documentation is scattered
âŒ GitHub activity is hard to measure manually
âŒ Wikipedia info is unreliable or outdated
âŒ LLMs hallucinate without verified data
âŒ Engineers rarely agree on the same evaluation criteria

ğŸ’¡ StackBench: The Solution

StackBench uses multiple autonomous AI agentsâ€”all running in parallelâ€”to perform:

ğŸ” 1. Evidence Collection

GitHub Stats â†’ â­ Stars | ğŸ´ Forks | ğŸ Issues | ğŸ‘¤ Contributors

Wikipedia Summary â†’ Clean, structured, fallback-resistant

ğŸ§ª 2. Verification

Cross-checks claims

Validates GitHub & Wiki evidence

Produces a confidence score

ğŸ§  3. Architectural Recommendation

Executive summary

Pros & Cons

CTO-level verdict

â™Š 4. Gemini Deep-Dive

A â€œSecond Opinionâ€ analysis that identifies blind spots and adds context.

ğŸ”¥ Core Features (with visual flair)
ğŸ§  Multi-Agent Architecture

ğŸ“Š Analyst Agent â€” Collects evidence and generates the technical summary

ğŸ” Verification Agent â€” Validates claims, reduces hallucinations

ğŸ›ï¸ Advisor Agent â€” Crafts the final architectural verdict

ğŸ”— A2A EventBus â€” Traceable, timestamped agent communication

ğŸ§© Real Integrations

âœ” GitHub REST API (stars, forks, issues, contributors)
âœ” Wikipedia Summary API (fallback logic + sanitization)
âœ” Groq Llama Models for blazing-fast LLM responses
âœ” Gemini Models for deeper insights
âœ” Simulator Mode when LLM keys are missing

ğŸ“¡ Live Observability Dashboard

Realtime Event Log with color-coded agents

Parallel Execution Timeline â€” visually shows concurrency

Per-Agent Metrics â€” API calls, latency, tasks

Confidence Scoring System

Microbenchmark Performance Charts (Altair)

âš™ï¸ Scalable Orchestration

True parallelism with ThreadPoolExecutor

Mission concurrency control

Queue/reject logic for demo hall stability

ğŸ“¦ Downloadable Report

Analyst Output

Verification Notes

Confidence Score

Advisor Recommendation

Evidence + Metrics

JSON Export

ğŸ›ï¸ Architecture Diagram
Streamlit UI
     â”‚
     â”‚  Start Mission
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Orchestrator     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€â”€â–º Analyst Agent â”€â”€â”€â”
     â”‚                       â”‚  A2A Messages
     â”œâ”€â”€â”€â”€â–º Verifier Agent â—„â”€â”˜
     â”‚
     â””â”€â”€â”€â”€â–º Advisor Agent

Agents â†’ EventBus (logs)
Metrics â†’ Monitor
Report â†’ JSON Output

ğŸ§± Tech Stack
Component	Technology
Frontend/UI	Streamlit
Agents	Python OOP Agents
LLM Backend	Groq, Gemini, Simulator
Evidence APIs	GitHub REST, Wikipedia
Parallelism	ThreadPoolExecutor
Observability	EventBus + Monitor
Charts	Altair
âš™ï¸ Setup Instructions
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Add Secrets

File: .streamlit/secrets.toml

GROQ_API_KEY=""
GITHUB_TOKEN=""
GEMINI_API_KEY=""
WIKI_API_KEY=""
UPTIME_URL=""
MAX_CONCURRENT_MISSIONS="3"
QUEUE_MISSIONS="true"

3ï¸âƒ£ Run the app
streamlit run app.py

ğŸ§ª How to Use StackBench
1. Enter a GitHub repo

Example:

pytorch/pytorch

2. Select a mission

Adoption Analysis

Code Audit

Migration Plan

3. Watch agents run in parallel

Analyst â†” Verifier show overlapping timeline bars.

4. Explore evidence

GitHub stats

Wikipedia summary

Agent logs

5. Ask Gemini for deeper assessment
6. Download the final JSON report
ğŸ“„ Example Output (Snapshot)
{
  "target": "streamlit/streamlit",
  "analyst_summary": "...",
  "verification_status": "Verified",
  "confidence_score": 92,
  "advisor_recommendation": "...",
  "github_stats": {},
  "metrics": {}
}

âš ï¸ Limitations

Wikipedia entries can be outdated

GitHub API rate limits apply

Fallback simulator used when LLM keys missing

ğŸ§­ Roadmap

Vector-based retrieval

Multi-mission analytics dashboard

Snippet-level code quality scoring

OpenAPI fact verification

ğŸ Conclusion

StackBench demonstrates excellence in:

âœ¨ Multi-Agent Collaboration
âœ¨ Evidence-Driven Analysis
âœ¨ Parallel Orchestration
âœ¨ Live Observability & Metrics
âœ¨ Gemini + Groq Hybrid Reasoning
âœ¨ Clean, Modern UI
